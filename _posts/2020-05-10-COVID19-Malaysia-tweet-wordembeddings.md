---
layout: post
title: COVID19 tweets - Using Word2Vec to capture semantic meaning
published: false
---

Word embedding is no doubt a powerful model for languge modelling. The ability to capture semantic meaning is one of the reson why it is use widely in neural networks. For the past week, I have been exploring BERT architechure. On of the bigest takeaway from my learning is that the model could not possibly works well without a good word embeddings output. It is because of words "similatrity" that makes the model able to contextually learn information. This understanding capture my interest to look more into word embeddings algorithm. My understanding is that, a good embedding model depends on 

## Interesting observations of dataset

## Train model

## Is it working?

## What if we add more data?

## Final analysis
